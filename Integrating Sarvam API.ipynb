{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Q3MC114H4xJz",
        "outputId": "0d4b87f4-7f72-4987-db00-3ef73e143f34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: librosa in c:\\users\\nmims.student\\appdata\\roaming\\python\\python312\\site-packages (0.10.2.post1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\nmims.student\\appdata\\roaming\\python\\python312\\site-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in c:\\users\\nmims.student\\appdata\\roaming\\python\\python312\\site-packages (from librosa) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\nmims.student\\appdata\\roaming\\python\\python312\\site-packages (from librosa) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\nmims.student\\appdata\\roaming\\python\\python312\\site-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: joblib>=0.14 in c:\\users\\nmims.student\\appdata\\roaming\\python\\python312\\site-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\nmims.student\\appdata\\roaming\\python\\python312\\site-packages (from librosa) (5.1.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in c:\\users\\nmims.student\\appdata\\roaming\\python\\python312\\site-packages (from librosa) (0.61.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\nmims.student\\appdata\\roaming\\python\\python312\\site-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in c:\\users\\nmims.student\\appdata\\roaming\\python\\python312\\site-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\nmims.student\\appdata\\roaming\\python\\python312\\site-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\users\\nmims.student\\appdata\\roaming\\python\\python312\\site-packages (from librosa) (4.12.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in c:\\users\\nmims.student\\appdata\\roaming\\python\\python312\\site-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in c:\\users\\nmims.student\\appdata\\roaming\\python\\python312\\site-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\nmims.student\\appdata\\roaming\\python\\python312\\site-packages (from lazy-loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\nmims.student\\appdata\\roaming\\python\\python312\\site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\nmims.student\\appdata\\roaming\\python\\python312\\site-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in c:\\users\\nmims.student\\appdata\\roaming\\python\\python312\\site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\nmims.student\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in c:\\users\\nmims.student\\appdata\\roaming\\python\\python312\\site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: pycparser in c:\\users\\nmims.student\\appdata\\roaming\\python\\python312\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nmims.student\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nmims.student\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nmims.student\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nmims.student\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "!pip install librosa\n",
        "import librosa\n",
        "import os\n",
        "\n",
        "# Function to extract features\n",
        "def extract_features(audio_file):\n",
        "    y, sr = librosa.load(audio_file, sr=None)\n",
        "    features = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=38).T, axis=0)\n",
        "    return features\n",
        "\n",
        "# Path to the WAV file\n",
        "audio_path = \"C:/Users/nmims.student/Downloads/Kimaya's Audio.wav\"\n",
        "\n",
        "# Extract features\n",
        "features = extract_features(audio_path)\n",
        "\n",
        "# Save to CSV\n",
        "df = pd.DataFrame(features).T  # Transpose to make it a single row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-318.375763</td>\n",
              "      <td>136.733627</td>\n",
              "      <td>20.810139</td>\n",
              "      <td>45.047409</td>\n",
              "      <td>-4.771146</td>\n",
              "      <td>27.556932</td>\n",
              "      <td>-3.196316</td>\n",
              "      <td>15.037029</td>\n",
              "      <td>2.432315</td>\n",
              "      <td>-2.289933</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.471559</td>\n",
              "      <td>0.50861</td>\n",
              "      <td>-4.296492</td>\n",
              "      <td>-1.692236</td>\n",
              "      <td>-2.544828</td>\n",
              "      <td>-0.938022</td>\n",
              "      <td>-3.195094</td>\n",
              "      <td>-0.604818</td>\n",
              "      <td>-0.066281</td>\n",
              "      <td>-2.088842</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 38 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0           1          2          3         4          5   \\\n",
              "0 -318.375763  136.733627  20.810139  45.047409 -4.771146  27.556932   \n",
              "\n",
              "         6          7         8         9   ...        28       29        30  \\\n",
              "0 -3.196316  15.037029  2.432315 -2.289933  ... -2.471559  0.50861 -4.296492   \n",
              "\n",
              "         31        32        33        34        35        36        37  \n",
              "0 -1.692236 -2.544828 -0.938022 -3.195094 -0.604818 -0.066281 -2.088842  \n",
              "\n",
              "[1 rows x 38 columns]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "YeaVeVR-4xJ1",
        "outputId": "c578c2de-7e1d-4f21-fe08-32f6c67832cb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "C:\\Users\\NMIMS.Student\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 1.6.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "C:\\Users\\NMIMS.Student\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.6.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array(['HINDI'], dtype=object)"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import load_model\n",
        "# ==============================\n",
        "# 1️⃣ Load Saved Model & Encoders\n",
        "# ==============================\n",
        "model = load_model(r\"C:\\Users\\nmims.student\\Downloads\\my_trained_hybrid_model_MHA_BILSTM_CNN.h5\")\n",
        "language_encoder = joblib.load(r\"C:\\Users\\nmims.student\\Downloads\\language_encoder_mha_bilstm_cnn.pkl\")\n",
        "train_feature_names = joblib.load(r\"C:\\Users\\nmims.student\\Downloads\\train_feature_names_mha_bilstm_cnn.pkl\")\n",
        "scaler = joblib.load(r\"C:\\Users\\nmims.student\\Downloads\\scaler_mha_bilstm_cnn.pkl\")\n",
        "\n",
        "sahitya=df\n",
        "sahitya.columns = [f\"feature_{i}\" for i in range(len(sahitya.columns))]\n",
        "sahitya_new = scaler.transform(sahitya)\n",
        "sahitya_reshaped_new = sahitya_new.reshape(sahitya_new.shape[0], sahitya_new.shape[1], 1)\n",
        "predictions = model.predict(sahitya_reshaped_new)\n",
        "predictions\n",
        "predicted_class_indices = np.argmax(predictions, axis=1)\n",
        "predicted_class_indices\n",
        "predicted_labels = language_encoder.inverse_transform(predicted_class_indices)\n",
        "predicted_labels\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "language_codes = [\"unknown\", \"hi-IN\", \"bn-IN\", \"kn-IN\", \"ml-IN\", \"mr-IN\", \"od-IN\", \"pa-IN\", \"ta-IN\", \"te-IN\", \"en-IN\", \"gu-IN\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'MARATHI'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predicted_labels[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWzeXk984xJ2",
        "outputId": "20f9dba3-d453-49ba-d2ab-b0be51bc144f"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# Sample: predicted label and dictionary (adjust with your actual values)\n",
        "language_dict={'HINDI': 'hi-IN', 'MARATHI': 'mr-IN'}\n",
        "\n",
        "# Validate predicted label\n",
        "#if predicted_labels[0] not in language_dict:\n",
        "    #raise ValueError(f\"Unknown language label: {predicted_labels[0]}\")\n",
        "\n",
        "# API setup\n",
        "url = \"https://api.sarvam.ai/speech-to-text\"\n",
        "file_path = \"/users/kimayashringarpure/Downloads/Kimaya's Audio.wav\"\n",
        "headers = {\n",
        "    'api-subscription-key': 'ff014cc1-c60d-4304-b097-76e135ab6ac1'\n",
        "}\n",
        "\n",
        "# Request payload\n",
        "payload = {\n",
        "    'model': 'saarika:v1',\n",
        "    'language_code': 'hi-IN',\n",
        "    #'language_code': language_dict[predicted_labels[0]],\n",
        "    'with_timesteps': 'false'\n",
        "}\n",
        "\n",
        "# Upload file\n",
        "with open(file_path, 'rb') as audio_file:\n",
        "    files = [\n",
        "        ('file', (file_path.split('\\\\')[-1], audio_file, 'audio/wav'))\n",
        "    ]\n",
        "    \n",
        "    response = requests.post(url, headers=headers, data=payload, files=files)\n",
        "\n",
        "# Response handling\n",
        "if response.status_code == 200:\n",
        "    print(f\"✅ Transcription success for language {predicted_labels[0]}:\\n{response.text}\")\n",
        "else:\n",
        "    print(f\"❌ Failed with status code {response.status_code}: {response.text}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"request_id\":\"20250406_04579ffb-7051-4a08-8bf3-d7ea35b46d31\",\"transcript\":\"यह कहानी के लेखक का नाम क्या है?\",\"language_code\":\"hi-IN\"}\n"
          ]
        }
      ],
      "source": [
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "kO_5EgXW4xJ3"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'streamlit'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstreamlit\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mst\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPyPDF2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PdfReader\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_text_splitters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'streamlit'"
          ]
        }
      ],
      "source": [
        "import streamlit as st\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "import os\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "import google.generativeai as genai\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from dotenv import load_dotenv\n",
        "from PyPDF2.errors import PdfReadError\n",
        "from googletrans import Translator\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "os.getenv(\"GOOGLE_API_KEY\")\n",
        "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
        "\n",
        "translator = Translator()\n",
        "\n",
        "def get_pdf_text(pdf_docs):\n",
        "    \"\"\"Extract text from uploaded PDF files, handling errors gracefully.\"\"\"\n",
        "    text = \"\"\n",
        "    for pdf in pdf_docs:\n",
        "        try:\n",
        "            pdf_reader = PdfReader(pdf)\n",
        "            for page in pdf_reader.pages:\n",
        "                text += page.extract_text()\n",
        "        except PdfReadError:\n",
        "            st.warning(f\"Unable to read the file: {pdf.name}. Skipping...\")\n",
        "        except Exception as e:\n",
        "            st.warning(f\"An error occurred while processing the file: {pdf.name}. Skipping...\")\n",
        "    return text\n",
        "\n",
        "def get_txt_text(txt_docs):\n",
        "    \"\"\"Extract text from uploaded TXT files, handling decoding errors gracefully.\"\"\"\n",
        "    text = \"\"\n",
        "    for txt_file in txt_docs:\n",
        "        try:\n",
        "            file_content = txt_file.read().decode(\"utf-8\")\n",
        "            text += file_content\n",
        "        except UnicodeDecodeError:\n",
        "            st.warning(f\"Unable to decode the file: {txt_file.name}. It may not be in UTF-8 format.\")\n",
        "        except Exception as e:\n",
        "            st.warning(f\"An unexpected error occurred while reading the file: {txt_file.name}. Skipping...\")\n",
        "    return text\n",
        "\n",
        "def get_text_chunks(text):\n",
        "    \"\"\"Split text into manageable chunks.\"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
        "    return text_splitter.split_text(text)\n",
        "\n",
        "def get_vector_store(text_chunks):\n",
        "    \"\"\"Create and save a FAISS index from text chunks.\"\"\"\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)\n",
        "    vector_store.save_local(\"faiss_index\")\n",
        "\n",
        "def get_conversational_chain():\n",
        "    \"\"\"Set up the conversational chain with a custom prompt.\"\"\"\n",
        "    prompt_template = \"\"\"\n",
        "    Answer the question as detailed as possible from the provided context. If the answer is not in\n",
        "    the provided context, just say, \"answer is not available in the context,\" without guessing.\\n\\n\n",
        "    Context:\\n{context}?\\n\n",
        "    Question:\\n{question}\\n\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "    model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.3)\n",
        "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "    return load_qa_chain(model, chain_type=\"stuff\", prompt=prompt)\n",
        "\n",
        "def translate_to_english(text):\n",
        "    \"\"\"Translate text from any Indian language to English.\"\"\"\n",
        "    try:\n",
        "        translated = translator.translate(text, dest=\"en\")\n",
        "        return translated.text\n",
        "    except Exception as e:\n",
        "        st.warning(\"Translation failed. Proceeding with original text.\")\n",
        "        return text\n",
        "\n",
        "def main():\n",
        "    st.set_page_config(\"Chatbot for Your Documents\")\n",
        "    st.header(\"Chatbot for Your Documents\")\n",
        "\n",
        "    if \"chat_history\" not in st.session_state:\n",
        "        st.session_state.chat_history = []\n",
        "\n",
        "    with st.sidebar:\n",
        "        st.title(\"Upload Files\")\n",
        "        uploaded_files = st.file_uploader(\n",
        "            \"Select your Pdf or Text files.\",\n",
        "            accept_multiple_files=True,\n",
        "            type=[\"pdf\", \"txt\"]\n",
        "        )\n",
        "        if uploaded_files:\n",
        "            with st.spinner(\"Processing files...\"):\n",
        "                text = \"\"\n",
        "                for file in uploaded_files:\n",
        "                    if file.name.endswith(\".pdf\"):\n",
        "                        text += get_pdf_text([file])\n",
        "                    elif file.name.endswith(\".txt\"):\n",
        "                        text += get_txt_text([file])\n",
        "                if text.strip():\n",
        "                    text_chunks = get_text_chunks(text)\n",
        "                    get_vector_store(text_chunks)\n",
        "                    st.success(\"Upload successful.\")\n",
        "                else:\n",
        "                    st.warning(\"Uploaded files did not contain any extractable text. Please try again.\")\n",
        "\n",
        "    for message in st.session_state.chat_history:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.markdown(message[\"content\"])\n",
        "\n",
        "    user_question = st.chat_input(\"Ask a question in any Indian language\")\n",
        "    if user_question:\n",
        "        st.session_state.chat_history.append({\"role\": \"user\", \"content\": user_question})\n",
        "\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.markdown(user_question)\n",
        "\n",
        "        translated_question = translate_to_english(user_question)\n",
        "\n",
        "        try:\n",
        "            embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "            index_path = \"faiss_index\"\n",
        "            if not os.path.exists(os.path.join(index_path, \"index.faiss\")):\n",
        "                raise FileNotFoundError(f\"FAISS index file not found at {index_path}/index.faiss. Please process the files first.\")\n",
        "            new_db = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
        "            docs = new_db.similarity_search(translated_question)\n",
        "            chain = get_conversational_chain()\n",
        "            response = chain({\"input_documents\": docs, \"question\": translated_question}, return_only_outputs=True)\n",
        "            assistant_response = response[\"output_text\"]\n",
        "        except FileNotFoundError as e:\n",
        "            assistant_response = f\"Error: {e}\"\n",
        "        except Exception as e:\n",
        "            assistant_response = f\"An unexpected error occurred: {e}\"\n",
        "\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            st.markdown(assistant_response)\n",
        "        st.session_state.chat_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "import os\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "import google.generativeai as genai\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from dotenv import load_dotenv\n",
        "from PyPDF2.errors import PdfReadError\n",
        "from googletrans import Translator\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "os.getenv(\"GOOGLE_API_KEY\")\n",
        "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
        "\n",
        "translator = Translator()\n",
        "\n",
        "def get_pdf_text(pdf_docs):\n",
        "    \"\"\"Extract text from uploaded PDF files, handling errors gracefully.\"\"\"\n",
        "    text = \"\"\n",
        "    for pdf in pdf_docs:\n",
        "        try:\n",
        "            pdf_reader = PdfReader(pdf)\n",
        "            for page in pdf_reader.pages:\n",
        "                text += page.extract_text()\n",
        "        except PdfReadError:\n",
        "            st.warning(f\"Unable to read the file: {pdf.name}. Skipping...\")\n",
        "        except Exception as e:\n",
        "            st.warning(f\"An error occurred while processing the file: {pdf.name}. Skipping...\")\n",
        "    return text\n",
        "\n",
        "def get_txt_text(txt_docs):\n",
        "    \"\"\"Extract text from uploaded TXT files, handling decoding errors gracefully.\"\"\"\n",
        "    text = \"\"\n",
        "    for txt_file in txt_docs:\n",
        "        try:\n",
        "            file_content = txt_file.read().decode(\"utf-8\")\n",
        "            text += file_content\n",
        "        except UnicodeDecodeError:\n",
        "            st.warning(f\"Unable to decode the file: {txt_file.name}. It may not be in UTF-8 format.\")\n",
        "        except Exception as e:\n",
        "            st.warning(f\"An unexpected error occurred while reading the file: {txt_file.name}. Skipping...\")\n",
        "    return text\n",
        "\n",
        "def get_text_chunks(text):\n",
        "    \"\"\"Split text into manageable chunks.\"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
        "    return text_splitter.split_text(text)\n",
        "\n",
        "def get_vector_store(text_chunks):\n",
        "    \"\"\"Create and save a FAISS index from text chunks.\"\"\"\n",
        "    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)\n",
        "    vector_store.save_local(\"faiss_index\")\n",
        "\n",
        "def get_conversational_chain():\n",
        "    \"\"\"Set up the conversational chain with a custom prompt.\"\"\"\n",
        "    prompt_template = \"\"\"\n",
        "    Answer the question as detailed as possible from the provided context. If the answer is not in\n",
        "    the provided context, just say, \"answer is not available in the context,\" without guessing.\\n\\n\n",
        "    Context:\\n{context}?\\n\n",
        "    Question:\\n{question}\\n\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "    model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.3)\n",
        "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "    return load_qa_chain(model, chain_type=\"stuff\", prompt=prompt)\n",
        "\n",
        "def translate_to_english(text):\n",
        "    \"\"\"Translate text from any Indian language to English.\"\"\"\n",
        "    try:\n",
        "        translated = translator.translate(text, dest=\"en\")\n",
        "        return translated.text\n",
        "    except Exception as e:\n",
        "        st.warning(\"Translation failed. Proceeding with original text.\")\n",
        "        return text\n",
        "\n",
        "def main():\n",
        "    st.set_page_config(\"Chatbot for Your Documents\")\n",
        "    st.header(\"Chatbot for Your Documents\")\n",
        "\n",
        "    if \"chat_history\" not in st.session_state:\n",
        "        st.session_state.chat_history = []\n",
        "\n",
        "    with st.sidebar:\n",
        "        st.title(\"Upload Files\")\n",
        "        uploaded_files = st.file_uploader(\n",
        "            \"Select your Pdf or Text files.\",\n",
        "            accept_multiple_files=True,\n",
        "            type=[\"pdf\", \"txt\"]\n",
        "        )\n",
        "        if uploaded_files:\n",
        "            with st.spinner(\"Processing files...\"):\n",
        "                text = \"\"\n",
        "                for file in uploaded_files:\n",
        "                    if file.name.endswith(\".pdf\"):\n",
        "                        text += get_pdf_text([file])\n",
        "                    elif file.name.endswith(\".txt\"):\n",
        "                        text += get_txt_text([file])\n",
        "                if text.strip():\n",
        "                    text_chunks = get_text_chunks(text)\n",
        "                    get_vector_store(text_chunks)\n",
        "                    st.success(\"Upload successful.\")\n",
        "                else:\n",
        "                    st.warning(\"Uploaded files did not contain any extractable text. Please try again.\")\n",
        "\n",
        "    for message in st.session_state.chat_history:\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.markdown(message[\"content\"])\n",
        "\n",
        "    user_question = st.chat_input(\"Ask a question in any Indian language\")\n",
        "    if user_question:\n",
        "        st.session_state.chat_history.append({\"role\": \"user\", \"content\": user_question})\n",
        "\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.markdown(user_question)\n",
        "\n",
        "        translated_question = translate_to_english(user_question)\n",
        "\n",
        "        try:\n",
        "            embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "            index_path = \"faiss_index\"\n",
        "            if not os.path.exists(os.path.join(index_path, \"index.faiss\")):\n",
        "                raise FileNotFoundError(f\"FAISS index file not found at {index_path}/index.faiss. Please process the files first.\")\n",
        "            new_db = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
        "            docs = new_db.similarity_search(translated_question)\n",
        "            chain = get_conversational_chain()\n",
        "            response = chain({\"input_documents\": docs, \"question\": translated_question}, return_only_outputs=True)\n",
        "            assistant_response = response[\"output_text\"]\n",
        "        except FileNotFoundError as e:\n",
        "            assistant_response = f\"Error: {e}\"\n",
        "        except Exception as e:\n",
        "            assistant_response = f\"An unexpected error occurred: {e}\"\n",
        "\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            st.markdown(assistant_response)\n",
        "        st.session_state.chat_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
